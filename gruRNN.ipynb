{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus        import gutenberg\n",
    "import pandas           as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =              'shakespeare-hamlet.txt'\n",
    "fileName =          'hamlet.txt'\n",
    "writeOperation =    'w'\n",
    "readOperation =     'r'\n",
    "\n",
    "with open(fileName, readOperation) as file:\n",
    "    textFromFile = file.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text        import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence    import pad_sequences\n",
    "from sklearn.model_selection                    import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([textFromFile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalWords = len(tokenizer.word_index) + 1\n",
    "totalWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSequences = []\n",
    "for line in textFromFile.split('\\n'):\n",
    "    tokenList = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(tokenList)):\n",
    "        sequences = tokenList[:i + 1]\n",
    "        inputSequences.append(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequenceLength = max([len(x) for x in inputSequences])\n",
    "sequenceLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSequences = np.array(\n",
    "    pad_sequences(\n",
    "        inputSequences,\n",
    "        maxlen = sequenceLength,\n",
    "        padding = 'pre'\n",
    "    )\n",
    ")\n",
    "\n",
    "inputSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labels and Predictions\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x = inputSequences[:,:-1]           # All the words expect the last word\n",
    "y = inputSequences[:, -1]           # Only the last word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the y output to a categorical value\n",
    "\n",
    "y = tf.keras.utils.to_categorical(y, num_classes = totalWords)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU RNN Model Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models        import Sequential\n",
    "from tensorflow.keras.layers        import Embedding, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks     import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedWords           = 100\n",
    "neurons                 = 150\n",
    "dropoutLayer            = 0.2\n",
    "activationFunction      = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(\n",
    "    monitor                 = 'val_loss',\n",
    "    patience                = 5,\n",
    "    restore_best_weights    = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(totalWords,selectedWords,input_length = sequenceLength - 1))\n",
    "\n",
    "model.add(GRU(neurons,return_sequences = True))\n",
    "model.add(Dropout(dropoutLayer))\n",
    "model.add(GRU(100))\n",
    "\n",
    "model.add(Dense(totalWords,activation = activationFunction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunction    = 'categorical_crossentropy'\n",
    "optimizer       = 'adam'\n",
    "metrics         = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss        = lossFunction,\n",
    "    optimizer   = optimizer,\n",
    "    metrics     = [metrics]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs = 100,\n",
    "    validation_data = (x_test, y_test),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict The Next Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNextWord(model, tokenizer, text, sequence_length):\n",
    "    tokenList = tokenizer.texts_to_sequences([text])[0]\n",
    "    \n",
    "    if len(tokenList) >= sequenceLength:\n",
    "        tokenList = tokenList[-(sequence_length - 1) : ]\n",
    "    \n",
    "    tokenList           = pad_sequences([tokenList], maxlen = sequence_length - 1, padding = 'pre')\n",
    "    predictedWord       = model.predict(tokenList, verbose = 0)\n",
    "    predictedWordIndex  = np.argmax(predictedWord, axis = 1)\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predictedWordIndex:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputText = \"To be or not to be\"\n",
    "maxSequenceLength = model.input_shape[1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextWord = predictNextWord(model, tokenizer, inputText, maxSequenceLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Input Text:         {inputText}')\n",
    "print(f'Predicted Word:     {nextWord}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerFileName       = 'tokenizerGRU.pickle'\n",
    "writeBinaryMode         = 'wb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('prediction_gru.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(tokenizerFileName, writeBinaryMode) as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
